# -*- coding: utf-8 -*-
"""Kaggle-Spam-Bert

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11k6itsGeZ4sGBYgIK1dXXQHcbUwj8we_

**Importing Libraries**
"""

import numpy as np
import pandas as pd
import seaborn as sns
import tensorflow as tf
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score, KFold
from sklearn.metrics import accuracy_score,confusion_matrix

"""**Dataset**"""

df=pd.read_csv('/content/train.csv')
df.head()

"""**Data Analysis**"""

df.shape

df.isnull().sum()

plt.figure(figsize=(8,8))
sns.heatmap(df.isnull())
plt.show()

sns.countplot(x='target',data=df,palette='deep')
plt.show()

"""**Splitting Data for Training and Testing**"""

x_train,x_test,y_train,y_test=train_test_split(df['text'],df['target'],stratify=df['target'],test_size=0.15,random_state=40)

print(x_train.shape,x_test.shape)

"""**Model Building and Training**"""

from transformers import BertTokenizer, TFBertForSequenceClassification

model_name = 'bert-base-uncased'
tokenizer = BertTokenizer.from_pretrained(model_name)
model = TFBertForSequenceClassification.from_pretrained(model_name,num_labels=2)

model

x_train=list(x_train)
y_train=y_train.to_numpy()

max_length = 50
train_encodings = tokenizer(x_train, truncation=True, padding=True, max_length=max_length,return_tensors='tf')

train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), y_train)).shuffle(100).batch(32)

optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')
model.compile(optimizer=optimizer, loss=loss, metrics=[metric])

model.fit(train_dataset, epochs=15)

"""**Test Data**"""

x_test=list(x_test)
test_encodings = tokenizer(x_test, truncation=True, padding=True, max_length=max_length,return_tensors='tf')

"""**Prediction**"""

logits = model.predict(test_encodings)[0]
probabilities = tf.nn.softmax(logits, axis=-1)
predicted_label = tf.argmax(probabilities, axis=-1).numpy()

print(accuracy_score(y_test,predicted_label))

sns.heatmap(confusion_matrix(y_test,predicted_label),annot=True,fmt='d')
plt.show()